{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIqcayB9GRDXg5AdrurrjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAFS20/Natural-Language-Processing/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK\n",
        "\n",
        "Por: Ricardo Alonzo Fernández Salguero\n",
        "\n",
        "La biblioteca Natural Language Toolkit (NLTK) es una de las bibliotecas más populares y ampliamente utilizadas en el campo del procesamiento de lenguaje natural (NLP). NLTK es una herramienta esencial para los investigadores, científicos de datos, ingenieros de software y lingüistas que trabajan con datos de texto y desean realizar tareas de procesamiento y análisis del lenguaje natural en Python.\n",
        "\n",
        "**1. Historia y Contexto**\n",
        "\n",
        "NLTK fue creado por Steven Bird y Edward Loper en la Universidad de Pensilvania en la década de 2000. Fue desarrollado como una herramienta de código abierto para la comunidad de investigación en lingüística computacional y procesamiento de lenguaje natural. Desde entonces, NLTK ha evolucionado y se ha convertido en una herramienta esencial en el campo del NLP, utilizada en la academia, la industria y proyectos de código abierto en todo el mundo.\n",
        "\n",
        "**2. Instalación**\n",
        "\n",
        "Para comenzar a usar NLTK, primero debes instalarlo en tu entorno de Python. Puedes instalarlo utilizando pip, el gestor de paquetes de Python, de la siguiente manera:\n",
        "\n",
        "```bash\n",
        "pip install nltk\n",
        "```\n",
        "\n",
        "Después de la instalación, puedes comenzar a utilizar NLTK en tus proyectos de procesamiento de lenguaje natural.\n",
        "\n",
        "**3. Características Principales**\n",
        "\n",
        "NLTK ofrece una amplia gama de características y herramientas para el procesamiento de lenguaje natural. Aquí hay una descripción general de algunas de sus características más destacadas:\n",
        "\n",
        "**3.1. Tokenización**\n",
        "\n",
        "NLTK proporciona herramientas para dividir el texto en unidades más pequeñas, como oraciones o palabras. La tokenización es un paso fundamental en el procesamiento de texto y es esencial para tareas como el análisis de sentimientos, la extracción de características y la construcción de modelos de lenguaje.\n",
        "\n",
        "\n",
        "**3.2. Stopwords**\n",
        "\n",
        "NLTK incluye una lista de palabras de detención (stopwords) en varios idiomas. Estas son palabras comunes que a menudo se eliminan durante el procesamiento de texto porque no aportan mucho significado.\n",
        "\n",
        "**3.3. Stemming y Lemmatización**\n",
        "\n",
        "NLTK proporciona algoritmos para reducir palabras a sus formas base o raíces (stemming) y para llevar palabras a su forma canónica (lemmatización). Estos procesos ayudan a reducir la variación en las palabras y simplifican el análisis de texto.\n",
        "\n",
        "**3.4. Análisis de Sentimientos**\n",
        "\n",
        "NLTK se puede utilizar para realizar análisis de sentimientos en textos. Puedes evaluar si un texto es positivo, negativo o neutral, lo que es útil en aplicaciones como el monitoreo de redes sociales y la retroalimentación del usuario.\n",
        "\n",
        "**3.5. Procesamiento de Lenguaje Natural en Idiomas Múltiples**\n",
        "\n",
        "NLTK admite una amplia variedad de idiomas y proporciona recursos y modelos pre-entrenados para trabajar con texto en diferentes lenguajes.\n",
        "\n",
        "**3.6. Clasificación de Texto**\n",
        "\n",
        "NLTK es útil para tareas de clasificación de texto, como la categorización de correos electrónicos como spam o no spam, la detección de opiniones en reseñas de productos y la clasificación de noticias en categorías.\n",
        "\n",
        "**3.7. Recursos Lingüísticos**\n",
        "\n",
        "NLTK ofrece acceso a una amplia gama de recursos lingüísticos, como corpus de texto etiquetado y diccionarios. Estos recursos son valiosos para entrenar modelos y realizar análisis de texto.\n",
        "\n",
        "**4. Aplicaciones de NLTK**\n",
        "\n",
        "NLTK se utiliza en una amplia variedad de aplicaciones del procesamiento de lenguaje natural. Algunas de las áreas en las que NLTK se aplica con éxito son:\n",
        "\n",
        "- **Análisis de Sentimientos:** Determinar la actitud emocional en textos, como reseñas de productos, comentarios en redes sociales y comentarios de usuarios.\n",
        "\n",
        "- **Clasificación de Texto:** Categorizar automáticamente textos en grupos o categorías, como detección de spam, clasificación de noticias y análisis de opiniones.\n",
        "\n",
        "- **Procesamiento de Lenguaje Natural en Idiomas Múltiples:** NLTK es versátil y se adapta a diversos idiomas, lo que lo hace valioso para aplicaciones globales.\n",
        "\n",
        "- **Extracción de Información:** Identificar y extraer información específica de textos, como nombres de personas, fechas, ubicaciones y eventos.\n",
        "\n",
        "- **Generación de Lenguaje Natural:** Generar texto automáticamente, útil en chatbots, resúmenes automáticos y más.\n",
        "\n",
        "**5. Comunidad y Contribuciones**\n",
        "\n",
        "NLTK es una biblioteca de código abierto y cuenta con una comunidad activa de desarrolladores y usuarios. La comunidad ha contribuido con una amplia gama de recursos y extensiones, incluidos modelos de lenguaje pre-entrenados, herramientas de procesamiento específicas de dominio y tutoriales.\n",
        "\n",
        "Además, NLTK fomenta la educación en procesamiento de lenguaje natural al proporcionar una amplia gama de materiales de aprendizaje, incluidos libros, tutoriales y ejemplos de código. Esto ha ayudado a que NLTK sea ampliamente adoptado en entornos académicos.\n",
        "\n",
        "**6. Limitaciones de NLTK**\n",
        "\n",
        "A pesar de sus muchas fortalezas, NLTK tiene algunas limitaciones:\n",
        "\n",
        "- **Rendimiento:** NLTK puede no ser la opción más rápida para tareas de procesamiento de lenguaje natural a gran escala. Para aplicaciones de producción que requieren un rendimiento excepcional, otras bibliotecas y herramientas como spaCy pueden ser más adecuadas.\n",
        "\n",
        "- **Aprendizaje automático avanzado:** Aunque NLTK ofrece funcionalidades básicas de aprendizaje automático, no se especializa en modelos avanzados de NLP, como transformers. Para aplicaciones que requieren modelos de última generación, se pueden considerar otras bibliotecas como Hugging Face Transformers.\n",
        "\n",
        "**7. Conclusión**\n",
        "\n",
        "Natural Language Toolkit (NLTK) es una biblioteca poderosa y versátil para el procesamiento de lenguaje natural en Python. Ofrece una amplia gama de herramientas y recursos que son esenciales para tareas de NLP, desde la tokenización hasta el análisis de sentimientos y la clasificación de texto. NLTK ha tenido un impacto significativo en la investigación y la aplicación del procesamiento de lenguaje natural y continúa siendo una herramienta valiosa para profesionales y entusiastas del NLP en todo el mundo.\n",
        "\n",
        "NLTK es una biblioteca esencial que ha allanado el camino para el procesamiento de lenguaje natural en Python, brindando a los usuarios las herramientas necesarias para comprender y analizar el texto en una amplia variedad de aplicaciones. Su rica historia, comunidad activa y recursos extensos hacen que NLTK sea una opción sólida para cualquier proyecto relacionado con el lenguaje natural."
      ],
      "metadata": {
        "id": "jAsjGZhYbkvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9tLLiJ8cBKv",
        "outputId": "41d6c595-90bd-41be-d5ad-332e841e857b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Descargar el tokenizador 'punkt'\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Descargar la lista de palabras de detención (stopwords) en español\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Descargar el recurso WordNet, que se utiliza en lematización y otras tareas\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Descargar recursos adicionales, como etiquetas POS (part-of-speech) y análisis de sentimientos\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Descargar corpus de textos en diferentes idiomas\n",
        "nltk.download('gutenberg')  # Corpus de libros en inglés\n",
        "nltk.download('cess_esp')   # Corpus de español\n",
        "\n",
        "nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im9Zdgu4cpTk",
        "outputId": "fd405e4c-0e74-407b-fbd9-872d6c7d3bef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK Python\n",
        "\n",
        "### Importación de Bibliotecas\n",
        "\n",
        "El código comienza con la importación de las bibliotecas necesarias:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "```\n",
        "\n",
        "- `nltk` es el acrónimo de Natural Language Toolkit, una biblioteca de Python ampliamente utilizada para el procesamiento de lenguaje natural.\n",
        "\n",
        "### Función `ensure_nltk_resources()`\n",
        "\n",
        "La primera función que se define en el código es `ensure_nltk_resources()`. Esta función se encarga de verificar si ciertos recursos de NLTK están descargados en tu sistema y, en caso contrario, los descarga. Los recursos que se verifican son: 'punkt', 'stopwords', 'wordnet', 'movie_reviews', 'gutenberg' y 'udhr'.\n",
        "\n",
        "```python\n",
        "def ensure_nltk_resources():\n",
        "    resources = ['punkt', 'stopwords', 'wordnet', 'movie_reviews', 'gutenberg', 'udhr']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.data.find(resource)\n",
        "        except LookupError:\n",
        "            nltk.download(resource)\n",
        "```\n",
        "\n",
        "- `resources` es una lista de nombres de recursos que se desean verificar o descargar.\n",
        "- Se utiliza un bucle `for` para iterar sobre los nombres de los recursos.\n",
        "- En cada iteración, se intenta encontrar el recurso utilizando `nltk.data.find(resource)`. Si el recurso no está disponible, se lanza una excepción `LookupError`.\n",
        "- En caso de que se lance una excepción, se utiliza `nltk.download(resource)` para descargar el recurso.\n",
        "\n",
        "### Función `tokenize_text(text)`\n",
        "\n",
        "La segunda función se llama `tokenize_text(text)`, y se encarga de dividir un texto en oraciones y palabras tokenizadas.\n",
        "\n",
        "```python\n",
        "def tokenize_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return sentences, words\n",
        "```\n",
        "\n",
        "- La función toma un argumento `text`, que es el texto que se desea tokenizar.\n",
        "- `nltk.sent_tokenize(text)` se utiliza para dividir el texto en oraciones. Esto significa que separamos el texto en partes que representan oraciones completas.\n",
        "- `nltk.word_tokenize(text)` se utiliza para dividir el texto en palabras individuales.\n",
        "\n",
        "La función devuelve dos listas: `sentences` (oraciones) y `words` (palabras).\n",
        "\n",
        "### Función `remove_stopwords(text, lang=\"english\")`\n",
        "\n",
        "La tercera función se llama `remove_stopwords(text, lang=\"english\")`, y se encarga de eliminar las palabras vacías (stopwords) de un texto.\n",
        "\n",
        "```python\n",
        "def remove_stopwords(text, lang=\"english\"):\n",
        "    stop_words = set(nltk.corpus.stopwords.words(lang))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "```\n",
        "\n",
        "- La función toma dos argumentos: `text`, que es el texto del cual se eliminarán las palabras vacías, y `lang` (idioma) que se utiliza para determinar las palabras vacías específicas del idioma. Por defecto, se establece en \"english\" (inglés).\n",
        "- Se obtiene una lista de palabras vacías específicas del idioma utilizando `nltk.corpus.stopwords.words(lang)`.\n",
        "- Luego, el texto se tokeniza en palabras usando `nltk.word_tokenize(text)`.\n",
        "- Se crea una nueva lista llamada `filtered_words` que contiene solo las palabras que no están en la lista de palabras vacías.\n",
        "\n",
        "La función devuelve la lista de palabras filtradas.\n",
        "\n",
        "### Función `stem_and_lemmatize(word)`\n",
        "\n",
        "La cuarta función se llama `stem_and_lemmatize(word)`, y se encarga de realizar tanto el stemming como la lematización de una palabra.\n",
        "\n",
        "```python\n",
        "def stem_and_lemmatize(word):\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, pos=\"v\")\n",
        "    return stemmed_word, lemmatized_word\n",
        "```\n",
        "\n",
        "- La función toma un argumento `word`, que es la palabra que se va a procesar.\n",
        "- Se crea un objeto `stemmer` que utiliza el algoritmo de Porter para el stemming. El stemming es el proceso de reducir una palabra a su forma base (o raíz).\n",
        "- Se crea un objeto `lemmatizer` que utiliza el lematizador de WordNet para la lematización. La lematización es el proceso de encontrar la forma base de una palabra, que puede ser una palabra real en el diccionario.\n",
        "- Se aplica el stemming a la palabra utilizando `stemmer.stem(word)` y se almacena en `stemmed_word`.\n",
        "- Se aplica la lematización a la palabra utilizando `lemmatizer.lemmatize(word, pos=\"v\")` con la especificación de que la palabra es un verbo (pos=\"v\"), y se almacena en `lemmatized_word`.\n",
        "\n",
        "La función devuelve dos palabras procesadas: la palabra reducida mediante stemming y la palabra lematizada.\n",
        "\n",
        "### Función `sentiment_analysis(text)`\n",
        "\n",
        "La quinta función se llama `sentiment_analysis(text)`, y se encarga de analizar el sentimiento en un texto utilizando TextBlob, una biblioteca que simplifica el análisis de sentimientos.\n",
        "\n",
        "```python\n",
        "def sentiment_analysis(text):\n",
        "    from textblob import TextBlob\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "```\n",
        "\n",
        "- La función toma un argumento `text`, que es el texto para el cual se desea realizar el análisis de sentimientos.\n",
        "- Se importa la clase `TextBlob` desde la biblioteca TextBlob.\n",
        "- Se crea un objeto `blob` a partir del texto utilizando `TextBlob(text)`.\n",
        "- Se utiliza `blob.sentiment.polarity` para obtener la polaridad del sentimiento, que indica si el texto es positivo (valor positivo), negativo (valor negativo) o neutral (valor cercano a cero).\n",
        "- Se utiliza `blob.sentiment.subjectivity` para obtener la subjetividad del sentimiento, que indica si el texto es objetivo (valor cercano a cero) o subjetivo (valor cercano a uno).\n",
        "\n",
        "La función devuelve dos valores: la polaridad y la subjetividad del sentimiento en el texto.\n",
        "\n",
        "### Función `classify_text()`\n",
        "\n",
        "La sexta función se llama `classify_text()`, y se encarga de realizar la clasificación de texto utilizando un clasificador Naive Bayes. Esta\n",
        "\n",
        " función realiza varias tareas:\n",
        "\n",
        "```python\n",
        "def classify_text():\n",
        "    documents = [(list(nltk.corpus.movie_reviews.words(fileid)), category)\n",
        "                 for category in nltk.corpus.movie_reviews.categories()\n",
        "                 for fileid in nltk.corpus.movie_reviews.fileids(category)]\n",
        "    random.shuffle(documents)\n",
        "    all_words = nltk.FreqDist(w.lower() for w in nltk.corpus.movie_reviews.words())\n",
        "    word_features = list(all_words)[:2000]\n",
        "\n",
        "    def document_features(document):\n",
        "        document_words = set(document)\n",
        "        features = {}\n",
        "        for word in word_features:\n",
        "            features[f'contains({word})'] = (word in document_words)\n",
        "        return features\n",
        "\n",
        "    featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "    train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "    classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
        "    return nltk.classify.accuracy(classifier, test_set)\n",
        "```\n",
        "\n",
        "- La función comienza creando una lista llamada `documents`. Esta lista contiene tuplas de la forma `(documento, categoría)`, donde `documento` es una lista de palabras tokenizadas pertenecientes a una revisión de película y `categoría` es la categoría de la revisión (positiva o negativa).\n",
        "- Se utiliza una comprensión de lista anidada para generar estas tuplas. Se recorren todas las categorías disponibles en las revisiones de películas y se recorren todos los archivos en cada categoría.\n",
        "- Luego, se mezclan aleatoriamente las tuplas en `documents` utilizando `random.shuffle(documents)`.\n",
        "- Se crea un objeto `all_words` que contiene todas las palabras en las revisiones de películas en minúsculas, junto con su frecuencia de aparición.\n",
        "- Se seleccionan las 2000 palabras más comunes y se almacenan en la lista `word_features`.\n",
        "\n",
        "A continuación, se define una función interna llamada `document_features(document)` que toma un documento (lista de palabras tokenizadas) y crea un diccionario de características. Cada característica representa si una palabra de `word_features` está presente en el documento. El nombre de cada característica se construye como `contains(word)`.\n",
        "\n",
        "```python\n",
        "    def document_features(document):\n",
        "        document_words = set(document)\n",
        "        features = {}\n",
        "        for word in word_features:\n",
        "            features[f'contains({word})'] = (word in document_words)\n",
        "        return features\n",
        "```\n",
        "\n",
        "- `document_words` es un conjunto que contiene todas las palabras únicas en el documento.\n",
        "- Se crea un diccionario vacío llamado `features` que almacenará las características.\n",
        "- Se itera a través de las palabras en `word_features` y se verifica si cada palabra está presente en el conjunto `document_words`. El resultado (True o False) se almacena como el valor de la característica correspondiente en el diccionario `features`.\n",
        "\n",
        "Después de definir la función `document_features`, se crea una lista de pares `(características, categoría)` llamada `featuresets` que contiene todas las características y sus categorías correspondientes.\n",
        "\n",
        "```python\n",
        "    featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "```\n",
        "\n",
        "- `document_features(d)` se llama para cada documento `d` en `documents`.\n",
        "- Luego, se divide el conjunto de datos en conjuntos de entrenamiento (`train_set`) y prueba (`test_set`) utilizando slicing.\n",
        "- Se entrena un clasificador Naive Bayes utilizando el conjunto de entrenamiento con `nltk.classify.NaiveBayesClassifier.train(train_set)`.\n",
        "- Finalmente, se calcula la precisión del clasificador en el conjunto de prueba utilizando `nltk.classify.accuracy(classifier, test_set)`.\n",
        "\n",
        "La función devuelve la precisión del clasificador en el conjunto de prueba.\n",
        "\n",
        "### Función Principal: `ensure_nltk_resources()`\n",
        "\n",
        "Después de definir todas las funciones anteriores, el código entra en la parte principal de la ejecución:\n",
        "\n",
        "```python\n",
        "# MAIN EXECUTION\n",
        "ensure_nltk_resources()\n",
        "```\n",
        "\n",
        "- En esta sección, se llama a la función `ensure_nltk_resources()` para asegurarse de que los recursos necesarios de NLTK estén descargados en el sistema antes de proceder con cualquier otra tarea.\n",
        "\n",
        "Este es el flujo principal del código. Después de ejecutar `ensure_nltk_resources()`, se pueden utilizar las otras funciones definidas para realizar diversas tareas de procesamiento de texto, incluyendo tokenización, eliminación de palabras vacías, análisis de sentimientos y clasificación de texto.\n",
        "\n",
        "Este código se puede integrar en una aplicación más grande para realizar tareas específicas de procesamiento de texto, como análisis de sentimientos de reseñas de películas o clasificación de texto en categorías específicas."
      ],
      "metadata": {
        "id": "ov0tWJ8_fXxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Verifica si los recursos ya están descargados, en caso contrario, los descarga.\n",
        "def ensure_nltk_resources():\n",
        "    resources = ['punkt', 'stopwords', 'wordnet', 'movie_reviews', 'gutenberg', 'udhr']\n",
        "    for resource in resources:\n",
        "        try:\n",
        "            nltk.data.find(resource)\n",
        "        except LookupError:\n",
        "            nltk.download(resource)\n",
        "\n",
        "def tokenize_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return sentences, words\n",
        "\n",
        "def remove_stopwords(text, lang=\"english\"):\n",
        "    stop_words = set(nltk.corpus.stopwords.words(lang))\n",
        "    words = nltk.word_tokenize(text)\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return filtered_words\n",
        "\n",
        "def stem_and_lemmatize(word):\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "    stemmed_word = stemmer.stem(word)\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, pos=\"v\")\n",
        "    return stemmed_word, lemmatized_word\n",
        "\n",
        "def sentiment_analysis(text):\n",
        "    from textblob import TextBlob\n",
        "    blob = TextBlob(text)\n",
        "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "def classify_text():\n",
        "    documents = [(list(nltk.corpus.movie_reviews.words(fileid)), category)\n",
        "                 for category in nltk.corpus.movie_reviews.categories()\n",
        "                 for fileid in nltk.corpus.movie_reviews.fileids(category)]\n",
        "\n",
        "    random.shuffle(documents)\n",
        "\n",
        "    all_words = nltk.FreqDist(w.lower() for w in nltk.corpus.movie_reviews.words())\n",
        "    word_features = list(all_words)[:2000]\n",
        "\n",
        "    def document_features(document):\n",
        "        document_words = set(document)\n",
        "        features = {}\n",
        "        for word in word_features:\n",
        "            features[f'contains({word})'] = (word in document_words)\n",
        "        return features\n",
        "\n",
        "    featuresets = [(document_features(d), c) for (d,c) in documents]\n",
        "    train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "    classifier = nltk.classify.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "    return nltk.classify.accuracy(classifier, test_set)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "ensure_nltk_resources()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwpyzXTBdleh",
        "outputId": "f7fc448c-d091-4060-d490-6a980ca6d4e9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Package udhr is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "text = \"NLTK es una biblioteca muy útil para el procesamiento de lenguaje natural. ¡Es ampliamente utilizada en la comunidad de NLP!\"\n",
        "sentences, words = tokenize_text(text)\n",
        "print(\"\\n=== Tokenización ===\")\n",
        "print(\"Frases:\", sentences)\n",
        "print(\"Palabras:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj-zvOqyeWW6",
        "outputId": "da57f793-d7fb-46aa-a88d-1bf568939200"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Tokenización ===\n",
            "Frases: ['NLTK es una biblioteca muy útil para el procesamiento de lenguaje natural.', '¡Es ampliamente utilizada en la comunidad de NLP!']\n",
            "Palabras: ['NLTK', 'es', 'una', 'biblioteca', 'muy', 'útil', 'para', 'el', 'procesamiento', 'de', 'lenguaje', 'natural', '.', '¡Es', 'ampliamente', 'utilizada', 'en', 'la', 'comunidad', 'de', 'NLP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Remove Stopwords\n",
        "text = \"El procesamiento de lenguaje natural es una disciplina emocionante.\"\n",
        "filtered_words = remove_stopwords(text, lang=\"spanish\")\n",
        "print(\"\\n=== Stopwords ===\")\n",
        "print(\"Palabras filtradas:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKAFQoemebxN",
        "outputId": "b691c88c-a7c7-490b-df6d-dbe1e1b34270"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Stopwords ===\n",
            "Palabras filtradas: ['procesamiento', 'lenguaje', 'natural', 'disciplina', 'emocionante', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Sentiment Analysis\n",
        "# Texto de ejemplo en inglés:\n",
        "text = \"\"\"\n",
        "The beauty of nature is beyond words. A serene, tranquil morning by the beach, watching the sun rise,\n",
        "can fill our heart with awe and wonder. On the other hand, the chaos and hardships of life often bring sorrow\n",
        "and pain. However, it's through the combination of highs and lows that life gets its true meaning.\n",
        "It's up to us to find positivity even in the most difficult situations.\n",
        "\"\"\"\n",
        "\n",
        "# Creando un objeto TextBlob:\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Extrayendo el análisis de sentimiento:\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "print(\"=== Análisis de Sentimientos ===\")\n",
        "print(\"Polaridad:\", sentiment.polarity)\n",
        "print(\"Subjetividad:\", sentiment.subjectivity)\n",
        "\n",
        "# Analizar cada oración individualmente:\n",
        "print(\"\\nSentiment Analysis for Each Sentence:\")\n",
        "for sentence in blob.sentences:\n",
        "    print(\"\\nFrase:\", sentence)\n",
        "    print(\"Polaridad:\", sentence.sentiment.polarity)\n",
        "    print(\"Subjetividad:\", sentence.sentiment.subjectivity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFPpdO1nehGW",
        "outputId": "06226f63-784a-4825-d847-2d2c961344b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Análisis de Sentimientos ===\n",
            "Polaridad: 0.056249999999999994\n",
            "Subjetividad: 0.63125\n",
            "\n",
            "Sentiment Analysis for Each Sentence:\n",
            "\n",
            "Frase: \n",
            "The beauty of nature is beyond words.\n",
            "Polaridad: 0.0\n",
            "Subjetividad: 0.0\n",
            "\n",
            "Frase: A serene, tranquil morning by the beach, watching the sun rise,\n",
            "can fill our heart with awe and wonder.\n",
            "Polaridad: 0.0\n",
            "Subjetividad: 0.0\n",
            "\n",
            "Frase: On the other hand, the chaos and hardships of life often bring sorrow\n",
            "and pain.\n",
            "Polaridad: -0.125\n",
            "Subjetividad: 0.375\n",
            "\n",
            "Frase: However, it's through the combination of highs and lows that life gets its true meaning.\n",
            "Polaridad: 0.35\n",
            "Subjetividad: 0.65\n",
            "\n",
            "Frase: It's up to us to find positivity even in the most difficult situations.\n",
            "Polaridad: 0.0\n",
            "Subjetividad: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming and Lemmatization\n",
        "word = \"running\"\n",
        "stemmed_word, lemmatized_word = stem_and_lemmatize(word)\n",
        "print(\"\\n=== Stemming y Lemmatización ===\")\n",
        "print(\"Palabra original:\", word)\n",
        "print(\"Stemmed Word:\", stemmed_word)\n",
        "print(\"Lemmatized Word:\", lemmatized_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGGWbDkgekPZ",
        "outputId": "93cb934f-ba2b-43d0-d372-5b643e0e34da"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Stemming y Lemmatización ===\n",
            "Palabra original: running\n",
            "Stemmed Word: run\n",
            "Lemmatized Word: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Text Classification\n",
        "accuracy = classify_text()\n",
        "print(\"\\n=== Clasificación de Texto ===\")\n",
        "print(\"Precisión del Clasificador:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3amvMLjepab",
        "outputId": "132e74dc-36a9-4296-ff38-4ffa1cd80560"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Clasificación de Texto ===\n",
            "Precisión del Clasificador: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Linguistic Resources\n",
        "emma = nltk.corpus.gutenberg.raw(\"austen-emma.txt\")\n",
        "print(\"\\n=== Recursos Lingüísticos ===\")\n",
        "print(\"Ejemplo de Texto de Emma de Jane Austen:\")\n",
        "print(emma[:300])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFUcsaOYeuWc",
        "outputId": "6c9106c2-f0a2-476d-8310-2e6a9b8d3ca1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Recursos Lingüísticos ===\n",
            "Ejemplo de Texto de Emma de Jane Austen:\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was t\n"
          ]
        }
      ]
    }
  ]
}