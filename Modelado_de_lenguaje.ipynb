{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAFS20/Natural-Language-Processing/blob/main/Modelado_de_lenguaje.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capítulo 3: Modelado de Lenguaje\n",
        "\n",
        "## 3.1 N-gramas y Modelos de Markov\n",
        "\n",
        "En este capítulo, exploraremos uno de los conceptos fundamentales en el modelado de lenguaje: los N-gramas y los modelos de Markov. Estas técnicas son esenciales en el procesamiento de lenguaje natural (PNL) para comprender la estructura y la probabilidad de secuencias de palabras en un texto.\n",
        "\n",
        "### 3.1.1 Definición de N-gramas\n",
        "\n",
        "Los N-gramas son secuencias de N palabras consecutivas tomadas de un corpus de texto. Por ejemplo, un 2-grama (también conocido como bigrama) sería una secuencia de dos palabras, mientras que un 3-grama (trigrama) sería una secuencia de tres palabras, y así sucesivamente. Los N-gramas se utilizan para modelar la probabilidad de ocurrencia de una palabra dada su historia (las N-1 palabras anteriores en la secuencia).\n",
        "\n",
        "#### Ejemplo:\n",
        "\n",
        "Consideremos el siguiente texto de ejemplo:\n",
        "\n",
        "```\n",
        "El gato está sobre la mesa.\n",
        "```\n",
        "\n",
        "Los 2-gramas en este texto serían:\n",
        "\n",
        "- \"El gato\"\n",
        "- \"gato está\"\n",
        "- \"está sobre\"\n",
        "- \"sobre la\"\n",
        "- \"la mesa\"\n",
        "\n",
        "### 3.1.2 Aplicaciones de N-gramas\n",
        "\n",
        "Los N-gramas tienen diversas aplicaciones en PNL, incluyendo:\n",
        "\n",
        "1. **Predicción de palabras:** Los N-gramas se utilizan para predecir la siguiente palabra en una secuencia de texto dada su historia.\n",
        "2. **Modelado de lenguaje:** Los N-gramas se utilizan para construir modelos de lenguaje que estiman la probabilidad de ocurrencia de una palabra en función de su contexto.\n",
        "3. **Corrección ortográfica:** Los N-gramas se utilizan en sistemas de corrección ortográfica para sugerir correcciones basadas en secuencias de palabras comunes.\n",
        "4. **Análisis de texto:** Los N-gramas se utilizan para identificar patrones y tendencias en grandes volúmenes de texto.\n",
        "\n",
        "### 3.1.3 Modelos de Markov\n",
        "\n",
        "Los modelos de Markov son procesos estocásticos que modelan la probabilidad de transición entre estados discretos en función del estado actual. En el contexto del modelado de lenguaje, los modelos de Markov se utilizan para modelar la probabilidad de transición entre palabras en una secuencia de texto.\n",
        "\n",
        "#### Matemáticas detrás de los Modelos de Markov\n",
        "\n",
        "Un modelo de Markov de primer orden (también conocido como modelo de Markov de orden 1 o modelo de Markov unigrama) asume que la probabilidad de transición entre estados (en este caso, palabras) depende únicamente del estado actual. Matemáticamente, esto se expresa como:\n",
        "\n",
        "$P(w_t | w_{t-1})$\n",
        "\n",
        "Donde:\n",
        "- $w_t$ es la palabra en el tiempo t.\n",
        "- $w_{t-1}$ es la palabra en el tiempo t-1.\n",
        "\n",
        "En un modelo de Markov de segundo orden (modelo de Markov bigrama), la probabilidad de transición depende de las dos palabras anteriores, y así sucesivamente para modelos de orden superior.\n",
        "\n",
        "#### Ejemplo de Modelos de Markov\n",
        "\n",
        "Consideremos el siguiente texto de ejemplo:\n",
        "\n",
        "```\n",
        "El gato está sobre la mesa.\n",
        "```\n",
        "\n",
        "Para construir un modelo de Markov de primer orden basado en este texto, calculamos las probabilidades de transición entre palabras consecutivas:\n",
        "\n",
        "```\n",
        "|       | El   | gato | está | sobre | la   | mesa |\n",
        "|-------|------|------|------|-------|------|------|\n",
        "| El    | 0    | 1.0  | 0    | 0     | 0    | 0    |\n",
        "| gato  | 0    | 0    | 1.0  | 0     | 0    | 0    |\n",
        "| está  | 0    | 0    | 0    | 1.0   | 0    | 0    |\n",
        "| sobre | 0    | 0    | 0    | 0     | 1.0  | 0    |\n",
        "| la    | 0    | 0    | 0    | 0     | 0    | 1.0  |\n",
        "| mesa  | 0    | 0    | 0    | 0     | 0    | 0    |\n",
        "```\n",
        "\n",
        "En este modelo de Markov, cada fila representa la palabra actual y cada columna representa la siguiente palabra. Por ejemplo, la celda en la fila \"gato\" y la columna \"está\" tiene un valor de 1.0, lo que indica que la palabra \"está\" sigue a \"gato\" en el texto de ejemplo con una probabilidad del 100%.\n",
        "\n",
        "### 3.1.4 Aplicaciones de los Modelos de Markov\n",
        "\n",
        "Los modelos de Markov tienen diversas aplicaciones en PNL, incluyendo:\n",
        "\n",
        "1. **Generación de texto:** Los modelos de Markov se utilizan para generar texto artificialmente que imita el estilo y la estructura del texto de entrenamiento.\n",
        "2. **Corrección gramatical:** Los modelos de Markov se utilizan en sistemas de corrección gramatical para identificar y corregir errores gramaticales en el texto.\n",
        "3. **Traducción automática:** Los modelos de Markov se utilizan en sistemas de traducción automática para modelar la probabilidad de transición entre palabras en diferentes idiomas.\n",
        "4. **Análisis de sentimiento:** Los modelos de Markov se utilizan en análisis de sentimiento para modelar la probabilidad de transición entre palabras asociadas con diferentes sentimientos.\n",
        "\n"
      ],
      "metadata": {
        "id": "t4cWctwKnVzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "At9SA7phnBTH",
        "outputId": "8d64cd75-6eb7-4e2d-9275-c197b17cd600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto simulado con modelo de Markov de primer orden:\n",
            "gato está debajo de la casa.\n",
            "\n",
            "Texto simulado con modelo de Markov de segundo orden:\n",
            "de la mesa. El pájaro está volando sobre la mesa. El pájaro está volando sobre la casa.\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "# Corpus de texto simulado\n",
        "corpus = \"\"\"\n",
        "El gato está sobre la mesa.\n",
        "El perro está debajo de la mesa.\n",
        "El pájaro está volando sobre la casa.\n",
        "\"\"\"\n",
        "\n",
        "# Función para tokenizar el texto en palabras\n",
        "def tokenize_text(text):\n",
        "    return text.split()\n",
        "\n",
        "# Función para generar N-gramas\n",
        "def generate_ngrams(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "# Función para construir un modelo de Markov\n",
        "def build_markov_model(tokens, order):\n",
        "    markov_model = defaultdict(list)\n",
        "    for i in range(len(tokens) - order):\n",
        "        history = tuple(tokens[i:i+order])\n",
        "        next_word = tokens[i+order]\n",
        "        markov_model[history].append(next_word)\n",
        "    return markov_model\n",
        "\n",
        "# Tokenizamos el corpus\n",
        "tokenized_corpus = tokenize_text(corpus)\n",
        "\n",
        "# Generamos 2-gramas y 3-gramas\n",
        "bigrams = generate_ngrams(tokenized_corpus, 2)\n",
        "trigrams = generate_ngrams(tokenized_corpus, 3)\n",
        "\n",
        "# Construimos modelos de Markov de primer y segundo orden\n",
        "markov_model_order_1 = build_markov_model(tokenized_corpus, 1)\n",
        "markov_model_order_2 = build_markov_model(tokenized_corpus, 2)\n",
        "\n",
        "# Función para generar texto simulado utilizando un modelo de Markov\n",
        "def generate_text(markov_model, seed=None, max_length=100):\n",
        "    if seed is None:\n",
        "        seed = random.choice(list(markov_model.keys()))\n",
        "    current_state = seed\n",
        "    generated_text = list(seed)\n",
        "\n",
        "    while len(generated_text) < max_length:\n",
        "        next_word_candidates = markov_model[current_state]\n",
        "        if not next_word_candidates:\n",
        "            break\n",
        "        next_word = random.choice(next_word_candidates)\n",
        "        generated_text.append(next_word)\n",
        "        current_state = tuple(generated_text[-len(current_state):])\n",
        "\n",
        "    return ' '.join(generated_text)\n",
        "\n",
        "# Generamos texto simulado utilizando los modelos de Markov\n",
        "print(\"Texto simulado con modelo de Markov de primer orden:\")\n",
        "print(generate_text(markov_model_order_1))\n",
        "\n",
        "print(\"\\nTexto simulado con modelo de Markov de segundo orden:\")\n",
        "print(generate_text(markov_model_order_2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Modelos de Lenguaje Neuronales\n",
        "\n",
        "Los modelos de lenguaje neuronales son fundamentales en el procesamiento de lenguaje natural (PNL), aprovechando técnicas de aprendizaje profundo para entender y generar texto de manera efectiva. Vamos a profundizar en cómo funcionan estos modelos, destacando las fórmulas matemáticas clave.\n",
        "\n",
        "#### 3.2.1 Representación Vectorial de Palabras\n",
        "\n",
        "Una parte esencial de los modelos de lenguaje neuronales es la representación de palabras. Una de las técnicas más comunes es la **incrustación de palabras** (word embedding), que mapea palabras a vectores densos en un espacio vectorial. Esto se puede expresar matemáticamente como:\n",
        "\n",
        "$$\n",
        "\\text{word\\_embedding}: \\text{Palabra} \\rightarrow \\mathbb{R}^d\n",
        "$$\n",
        "\n",
        "donde $d$ es la dimensión del espacio vectorial de incrustación.\n",
        "\n",
        "#### 3.2.2 Modelado de Secuencias\n",
        "\n",
        "Los modelos de lenguaje neuronales también deben modelar secuencias de palabras. Una arquitectura común para esto es la red neuronal recurrente (RNN), que procesa secuencias de entrada paso a paso. La salida de cada paso temporal se utiliza como entrada para el siguiente. Esto se puede expresar matemáticamente como:\n",
        "\n",
        "$$\n",
        "h_t = \\text{RNN}(h_{t-1}, x_t)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $h_t$ es el estado oculto en el tiempo $t$,\n",
        "- $x_t$ es la entrada en el tiempo $t$.\n",
        "\n",
        "#### 3.2.3 Modelos de Lenguaje Neuronales\n",
        "\n",
        "Los modelos de lenguaje neuronales utilizan representaciones vectoriales de palabras y modelos de secuencias para predecir la probabilidad de una palabra dada su historia. Esto se puede expresar con la regla de la cadena y el modelo generativo:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, ..., w_n) = \\prod_{t=1}^{n} P(w_t | w_1, w_2, ..., w_{t-1})\n",
        "$$\n",
        "\n",
        "donde $w_t$ es la palabra en el tiempo $t$.\n",
        "\n",
        "### 3.2.4 Implementación Práctica\n",
        "\n",
        "Para implementar un modelo de lenguaje neuronal, necesitamos definir la arquitectura de la red y la función de pérdida. La función de pérdida más común para modelos de lenguaje es la entropía cruzada. Para entrenar el modelo, utilizamos un algoritmo de optimización como el descenso de gradiente estocástico (SGD).\n",
        "\n",
        "El objetivo durante el entrenamiento es minimizar la pérdida del modelo, que es la discrepancia entre las predicciones del modelo y los objetivos reales. Esto se puede expresar matemáticamente como:\n",
        "\n",
        "$$\n",
        "\\text{Pérdida} = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{ij} \\log(p_{ij})\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $N$ es el número total de ejemplos de entrenamiento,\n",
        "- $M$ es el tamaño del vocabulario,\n",
        "- $y_{ij}$ es 1 si la palabra $j$ es el siguiente token en el ejemplo $i$, de lo contrario 0,\n",
        "- $p_{ij}$ es la probabilidad predicha de que la palabra $j$ sea el siguiente token en el ejemplo $i$.\n",
        "\n",
        "\n",
        "### 3.2 Modelos de Lenguaje Neuronales con LSTM\n",
        "\n",
        "En esta sección, nos centraremos en un tipo específico de red neuronal recurrente (RNN) llamada Memoria a Corto y Largo Plazo (LSTM), utilizada ampliamente en modelos de lenguaje neuronales debido a su capacidad para capturar dependencias a largo plazo en secuencias de texto.\n",
        "\n",
        "#### 3.2.1 Introducción a las LSTM\n",
        "\n",
        "Las LSTM son un tipo de RNN diseñadas para superar el problema de desvanecimiento del gradiente (vanishing gradient) que afecta a las RNN tradicionales. Este problema surge cuando se propagan los gradientes a través de muchas capas temporales durante el entrenamiento y se vuelven muy pequeños, lo que dificulta el aprendizaje de dependencias a largo plazo.\n",
        "\n",
        "#### 3.2.2 Arquitectura de una Celda LSTM\n",
        "\n",
        "Una celda LSTM consta de varias puertas (gates) que controlan el flujo de información dentro de la celda. Estas puertas incluyen:\n",
        "\n",
        "- **Puerta de Olvido (Forget Gate):** Decide qué información olvidar del estado de celda anterior.\n",
        "- **Puerta de Entrada (Input Gate):** Decide qué nueva información almacenar en el estado de celda.\n",
        "- **Puerta de Salida (Output Gate):** Decide qué parte del estado de celda actual se convertirá en la salida.\n",
        "\n",
        "La arquitectura de una celda LSTM se puede expresar matemáticamente como:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "f_t &= \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f) \\\\\n",
        "i_t &= \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i) \\\\\n",
        "\\tilde{C}_t &= \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C) \\\\\n",
        "C_t &= f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t \\\\\n",
        "o_t &= \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o) \\\\\n",
        "h_t &= o_t \\odot \\tanh(C_t)\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $f_t$ es el vector de activación de la puerta de olvido en el tiempo $t$,\n",
        "- $i_t$ es el vector de activación de la puerta de entrada en el tiempo $t$,\n",
        "- $\\tilde{C}_t$ es el vector de activación de la celda de estado candidato en el tiempo $t$,\n",
        "- $C_t$ es el vector de estado de celda en el tiempo $t$,\n",
        "- $o_t$ es el vector de activación de la puerta de salida en el tiempo $t$,\n",
        "- $h_t$ es el vector de salida en el tiempo $t$,\n",
        "- $W_f, W_i, W_C, W_o$ son matrices de pesos,\n",
        "- $b_f, b_i, b_C, b_o$ son vectores de sesgo,\n",
        "- $\\sigma$ es la función de activación sigmoide,\n",
        "- $\\odot$ representa la multiplicación elemento por elemento.\n",
        "\n",
        "#### 3.2.3 Aplicación en Modelos de Lenguaje Neuronales\n",
        "\n",
        "En el contexto de modelos de lenguaje neuronales, las LSTM se utilizan para modelar secuencias de palabras y capturar dependencias a largo plazo en el texto. La entrada a una LSTM en un modelo de lenguaje es típicamente una secuencia de incrustaciones de palabras. La LSTM procesa secuencialmente estas incrustaciones de palabras y genera una distribución de probabilidad sobre las palabras del vocabulario en cada paso de tiempo.\n",
        "\n",
        "La arquitectura general de un modelo de lenguaje neuronal con LSTM se puede visualizar de la siguiente manera:\n",
        "\n",
        "$$\n",
        "\\text{Palabra}_1 \\rightarrow \\text{Embedding} \\rightarrow \\text{LSTM} \\rightarrow \\text{...} \\rightarrow \\text{Palabra}_n\n",
        "$$"
      ],
      "metadata": {
        "id": "xkWl8U8HpE_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Corpus de texto simulado\n",
        "corpus = [\n",
        "    \"el gato está sobre la mesa\",\n",
        "    \"el perro está debajo de la mesa\",\n",
        "    \"el pájaro está volando sobre la casa\"\n",
        "]\n",
        "\n",
        "# Tokenización del texto\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Creación de secuencias de entrada y salida\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Padding de secuencias\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Creación de datos de entrada y salida\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)\n",
        "\n",
        "# Definición del modelo de lenguaje neuronal\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(total_words, 100, input_length=max_sequence_len-1),\n",
        "    LSTM(150),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compilación del modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
        "\n",
        "# Función para generar texto a partir del modelo entrenado\n",
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len, padding='pre')  # Corrección aquí\n",
        "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Generación de texto simulado\n",
        "print(generate_text(\"el gato\", 5, model, max_sequence_len-1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCH4s6PwoBwt",
        "outputId": "44525fc3-d70a-4dca-e8f1-26f4a629f9df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.5602 - accuracy: 0.0000e+00\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5494 - accuracy: 0.2353\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.5383 - accuracy: 0.2941\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5266 - accuracy: 0.2941\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.5140 - accuracy: 0.2941\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5002 - accuracy: 0.2941\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4850 - accuracy: 0.2353\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4680 - accuracy: 0.2353\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.4489 - accuracy: 0.1765\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.4272 - accuracy: 0.1765\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4027 - accuracy: 0.1765\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.3750 - accuracy: 0.1765\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.3441 - accuracy: 0.1765\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.3101 - accuracy: 0.1765\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.2743 - accuracy: 0.1765\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.2391 - accuracy: 0.1765\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.2086 - accuracy: 0.1765\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.1854 - accuracy: 0.1765\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.1645 - accuracy: 0.1765\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.1358 - accuracy: 0.2941\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0967 - accuracy: 0.2941\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 2.0512 - accuracy: 0.2941\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.0039 - accuracy: 0.3529\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9563 - accuracy: 0.3529\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9075 - accuracy: 0.4118\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8558 - accuracy: 0.4118\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8009 - accuracy: 0.4118\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.7443 - accuracy: 0.4118\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.6888 - accuracy: 0.4118\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.6359 - accuracy: 0.4118\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 1.5841 - accuracy: 0.4118\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5305 - accuracy: 0.4118\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.4759 - accuracy: 0.4118\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4255 - accuracy: 0.4118\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3810 - accuracy: 0.4706\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3353 - accuracy: 0.4706\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2874 - accuracy: 0.5294\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2453 - accuracy: 0.5294\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.2049 - accuracy: 0.5294\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1623 - accuracy: 0.5294\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1243 - accuracy: 0.5882\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0852 - accuracy: 0.5882\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0487 - accuracy: 0.7647\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0183 - accuracy: 0.7647\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9874 - accuracy: 0.7059\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9602 - accuracy: 0.7059\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9292 - accuracy: 0.7059\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9020 - accuracy: 0.7059\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8740 - accuracy: 0.6471\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8485 - accuracy: 0.6471\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8224 - accuracy: 0.7059\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7964 - accuracy: 0.8235\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7733 - accuracy: 0.8235\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7497 - accuracy: 0.8235\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7274 - accuracy: 0.7647\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7059 - accuracy: 0.7647\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6843 - accuracy: 0.8235\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6642 - accuracy: 0.8235\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6460 - accuracy: 0.7647\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6288 - accuracy: 0.8235\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6122 - accuracy: 0.8235\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5963 - accuracy: 0.8235\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5796 - accuracy: 0.8235\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5628 - accuracy: 0.8824\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5467 - accuracy: 0.8235\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5320 - accuracy: 0.8824\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5186 - accuracy: 0.8824\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.5077 - accuracy: 0.8235\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.5026 - accuracy: 0.8824\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4964 - accuracy: 0.8235\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.4770 - accuracy: 0.8824\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4579 - accuracy: 0.8824\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4590 - accuracy: 0.8235\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4452 - accuracy: 0.8824\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4288 - accuracy: 0.8824\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.4297 - accuracy: 0.8824\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4133 - accuracy: 0.8824\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4054 - accuracy: 0.8824\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4012 - accuracy: 0.8824\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.3863 - accuracy: 0.8824\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3844 - accuracy: 0.8824\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3733 - accuracy: 0.8824\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3667 - accuracy: 0.8824\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3614 - accuracy: 0.8824\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.3515 - accuracy: 0.8824\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3485 - accuracy: 0.8824\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3390 - accuracy: 0.8824\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.3349 - accuracy: 0.8824\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3280 - accuracy: 0.8824\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.3223 - accuracy: 0.8824\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3176 - accuracy: 0.8824\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3112 - accuracy: 0.8824\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3076 - accuracy: 0.8824\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.3013 - accuracy: 0.8824\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2980 - accuracy: 0.8824\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2925 - accuracy: 0.8824\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.2892 - accuracy: 0.8824\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2843 - accuracy: 0.8824\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.2810 - accuracy: 0.8824\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2768 - accuracy: 0.8824\n",
            "1/1 [==============================] - 0s 432ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "el gato está sobre la mesa casa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Capítulo 3: Modelos de Lenguaje Basados en Transformadores\n",
        "\n",
        "En este capítulo, exploraremos los modelos de lenguaje basados en transformadores, una innovadora arquitectura que ha revolucionado el campo del procesamiento del lenguaje natural (PNL). Comenzaremos explicando los fundamentos teóricos detrás de los transformadores y cómo funcionan en el contexto de los modelos de lenguaje. Luego, presentaremos ejemplos prácticos de implementación en Python utilizando la biblioteca TensorFlow.\n",
        "\n",
        "## 3.3 Fundamentos de los Transformadores\n",
        "\n",
        "Los transformadores son una arquitectura de red neuronal que se basa en el mecanismo de atención para procesar secuencias de datos. La atención permite a la red enfocarse en partes específicas de la entrada, lo que la hace especialmente efectiva para tareas que involucran secuencias largas, como la traducción automática y la generación de texto.\n",
        "\n",
        "### 3.3.1 Mecanismo de Atención\n",
        "\n",
        "El mecanismo de atención permite que el modelo asigne diferentes pesos a diferentes partes de la entrada, centrándose en las partes más relevantes para la tarea en cuestión. Matemáticamente, la atención se calcula mediante una combinación lineal de los vectores de consulta, clave y valor, seguida de una función de activación softmax. Esto se puede expresar como:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $Q$, $K$ y $V$ son matrices de consulta, clave y valor, respectivamente,\n",
        "- $d_k$ es la dimensión de las matrices de clave.\n",
        "\n",
        "### 3.3.2 Transformador\n",
        "\n",
        "La arquitectura del transformador se basa en el uso repetido de bloques de atención, llamados capas de atención, que permiten capturar relaciones de dependencia a largo plazo en las secuencias de entrada. Estas capas de atención están interconectadas mediante conexiones residuales y normalización por capa.\n",
        "\n",
        "#### 3.3.2.1 Bloque de Atención\n",
        "\n",
        "Cada bloque de atención en un transformador consta de múltiples cabezas de atención, que permiten al modelo capturar diferentes representaciones de la entrada en paralelo. La salida de cada cabeza de atención se concatena y se proyecta linealmente para producir la salida final del bloque. Matemáticamente, la salida de un bloque de atención se calcula como:\n",
        "\n",
        "$$\n",
        "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $\\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$ es la salida de la $i$-ésima cabeza de atención,\n",
        "- $W^Q_i$, $W^K_i$ y $W^V_i$ son matrices de pesos para la $i$-ésima cabeza de atención,\n",
        "- $W^O$ es la matriz de proyección para combinar las salidas de todas las cabezas de atención.\n",
        "\n",
        "#### 3.3.2.2 Capa de Normalización y Conexión Residual\n",
        "\n",
        "Cada bloque de atención en un transformador está seguido por una capa de normalización y una conexión residual. La capa de normalización ayuda a estabilizar el entrenamiento, y la conexión residual permite que el gradiente se propague más fácilmente a través de las capas. Matemáticamente, esto se puede expresar como:\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\text{LayerNorm}(X + \\text{MultiHead}(Q, K, V))\n",
        "$$\n",
        "\n",
        "donde:\n",
        "- $X$ es la entrada al bloque de atención,\n",
        "- $\\text{LayerNorm}$ es la capa de normalización.\n",
        "\n",
        "### 3.3.3 Modelo de Lenguaje Basado en Transformadores\n",
        "\n",
        "Un modelo de lenguaje basado en transformadores utiliza la arquitectura de transformador para modelar secuencias de texto y predecir la probabilidad de la siguiente palabra en una secuencia dada una historia de palabras anteriores. La entrada a un transformador es típicamente una secuencia de incrustaciones de palabras, y la salida es una distribución de probabilidad sobre las palabras del vocabulario."
      ],
      "metadata": {
        "id": "2dKZ80S5q569"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Embedding, Masking, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Corpus de texto simulado\n",
        "corpus = \"\"\"\n",
        "El gato está sobre la mesa.\n",
        "El perro está debajo de la mesa.\n",
        "El pájaro está volando sobre la casa.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenizar el corpus\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Crear secuencias de entrada y salida\n",
        "input_sequences = []\n",
        "for line in corpus.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
        "\n",
        "# Crear datos de entrada y salida\n",
        "X = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]\n",
        "\n",
        "# Convertir la salida a one-hot encoding\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n",
        "\n",
        "# Definir el modelo\n",
        "input_layer = Input(shape=(max_sequence_len - 1,))\n",
        "embedding_layer = Embedding(total_words, 100)(input_layer)\n",
        "masking_layer = Masking(mask_value=0.0)(embedding_layer)  # Masking para manejar secuencias de longitud variable\n",
        "lstm_layer = LSTM(150)(masking_layer)\n",
        "dropout_layer = Dropout(0.2)(lstm_layer)\n",
        "output_layer = Dense(total_words, activation='softmax')(dropout_layer)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Función para predecir la siguiente palabra\n",
        "def generate_text(seed_text, next_words):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_index = tf.argmax(predicted_probs, axis=-1).numpy()\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "    return seed_text\n",
        "\n",
        "# Generar texto\n",
        "generated_text = generate_text(\"El gato\", 5)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHORGgoEq9p0",
        "outputId": "9d678906-a482-48d2-eaa7-77d1e7cfba39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.5621 - accuracy: 0.0588\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.5517 - accuracy: 0.2941\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.5415 - accuracy: 0.3529\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.5317 - accuracy: 0.3529\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.5221 - accuracy: 0.2941\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.5050 - accuracy: 0.3529\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.5006 - accuracy: 0.2941\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.4850 - accuracy: 0.2353\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.4646 - accuracy: 0.2353\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.4442 - accuracy: 0.2353\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4279 - accuracy: 0.1765\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.4061 - accuracy: 0.2353\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.3832 - accuracy: 0.1765\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.3637 - accuracy: 0.1765\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.3317 - accuracy: 0.1765\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.2902 - accuracy: 0.1765\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.2457 - accuracy: 0.1765\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 2.2192 - accuracy: 0.1765\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.1973 - accuracy: 0.1765\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.2244 - accuracy: 0.1765\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.2105 - accuracy: 0.2353\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.1310 - accuracy: 0.1765\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 2.1569 - accuracy: 0.2353\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 2.1218 - accuracy: 0.3529\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 2.0933 - accuracy: 0.2941\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0505 - accuracy: 0.3529\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9847 - accuracy: 0.4118\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.9937 - accuracy: 0.4118\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.9420 - accuracy: 0.4118\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.8965 - accuracy: 0.4118\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.8725 - accuracy: 0.4118\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 1.8181 - accuracy: 0.4118\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.7346 - accuracy: 0.4118\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.7367 - accuracy: 0.4118\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.6715 - accuracy: 0.4118\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5959 - accuracy: 0.4118\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5291 - accuracy: 0.4118\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.5713 - accuracy: 0.4706\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.5156 - accuracy: 0.4118\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.4868 - accuracy: 0.4118\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.3456 - accuracy: 0.4706\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.4655 - accuracy: 0.4706\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.3733 - accuracy: 0.4706\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3511 - accuracy: 0.5294\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.3164 - accuracy: 0.4706\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2786 - accuracy: 0.4706\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1299 - accuracy: 0.5882\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.2027 - accuracy: 0.5294\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 1.1907 - accuracy: 0.5294\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1218 - accuracy: 0.6471\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1734 - accuracy: 0.5882\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.0509 - accuracy: 0.7059\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0682 - accuracy: 0.6471\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0244 - accuracy: 0.5882\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9664 - accuracy: 0.6471\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9582 - accuracy: 0.7059\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7601 - accuracy: 0.8235\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9541 - accuracy: 0.6471\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9362 - accuracy: 0.6471\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9994 - accuracy: 0.6471\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0233 - accuracy: 0.6471\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.8676 - accuracy: 0.6471\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.8302 - accuracy: 0.8235\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.8592 - accuracy: 0.6471\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.7834 - accuracy: 0.7059\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.8077 - accuracy: 0.7059\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6707 - accuracy: 0.8235\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7026 - accuracy: 0.8235\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.8100 - accuracy: 0.6471\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7216 - accuracy: 0.7059\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5875 - accuracy: 0.8824\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6279 - accuracy: 0.8235\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7506 - accuracy: 0.7059\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6668 - accuracy: 0.7059\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6345 - accuracy: 0.8235\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6121 - accuracy: 0.7647\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5937 - accuracy: 0.8235\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5483 - accuracy: 0.8235\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5432 - accuracy: 0.8824\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5210 - accuracy: 0.8824\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.5684 - accuracy: 0.7647\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.5979 - accuracy: 0.7059\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.5915 - accuracy: 0.7059\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5712 - accuracy: 0.8235\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.5514 - accuracy: 0.7647\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4552 - accuracy: 0.9412\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.5269 - accuracy: 0.8235\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5503 - accuracy: 0.8235\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4746 - accuracy: 0.8824\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4330 - accuracy: 0.8824\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.4785 - accuracy: 0.8824\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5427 - accuracy: 0.7059\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.3733 - accuracy: 0.8824\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4495 - accuracy: 0.7647\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.4379 - accuracy: 0.8235\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.4556 - accuracy: 0.7647\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.4365 - accuracy: 0.8235\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4979 - accuracy: 0.8235\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4346 - accuracy: 0.8235\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.3480 - accuracy: 0.8824\n",
            "El gato está sobre la mesa casa\n"
          ]
        }
      ]
    }
  ]
}